{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is an alternate way of making the input data pipeline using Keras Image Data Generator- flow_from_directory. Eases dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Implementation of custom metrics: Precision, Recall, F-Measure and Confusion Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        \n",
    "        for i in range(len(self.validation_data)):\n",
    "            x_val, y_val = self.validation_data.__getitem__(i)\n",
    "\n",
    "        print(y_val.shape)\n",
    "        print(y_val[1])\n",
    "        print(y_val[2])\n",
    "\n",
    "        y_predict = np.asarray(model.predict(self.validation_data, steps = 1))\n",
    "        print(y_predict.shape)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            lab = [i for i in range(0,75)]\n",
    "            print(y_val.shape)\n",
    "\n",
    "            y_val = np.argmax(y_val, axis=1)\n",
    "            y_predict = np.argmax(y_predict, axis=1)\n",
    "            print(\"y_val: \", y_val)\n",
    "            print(\"y_predict:\", y_predict)\n",
    "            print(\"\\nMetrics for Epoch\")\n",
    "            print(\"Confusion Matrix:\\n\",confusion_matrix(y_val,y_predict, labels=lab))\n",
    "            print(\"Recall: \", recall_score(y_val,y_predict, average=None, labels=lab))\n",
    "            print(\"Precision: \", precision_score(y_val,y_predict, average = None, labels=lab))\n",
    "            print(\"F1_score: \", f1_score(y_val,y_predict, average =None, labels=lab))\n",
    "            print(\"\\n\") \n",
    "            self._data.append({\n",
    "                'val_recall': recall_score(y_val, y_predict, average = None, labels=lab),\n",
    "                'val_precision': precision_score(y_val, y_predict, average = None, labels=lab),\n",
    "                'val_f1_score': f1_score(y_val,y_predict, average = None, labels=lab),\n",
    "            })\n",
    "            return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementing Learning Rate Scheduler. Need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch,lr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Learning rate scheduler decays the learning rate by factor of 0.1 every 10 epochs after 20 epochs\n",
    "    \"\"\"\n",
    "    decay_rate = 0.1\n",
    "    if epoch==20:\n",
    "        return lr*decay_rate\n",
    "    elif epoch%10==0 and epoch >20:\n",
    "        return lr*decay_rate\n",
    "    return lr\n",
    "\n",
    "# To put in the model, put it inside callbacks\n",
    "LRScheduler = keras.callbacks.LearningRateScheduler(lr_scheduler,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generator for Data augmentation. Edit possible\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.0017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check for augmentation\n",
    "\n",
    "img = load_img('../datasets/large_sample/almond_desert/6350.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "print(x.shape)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in train_datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='almond_desert', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../datasets/large_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = train_datagen.flow_from_directory(\n",
    "        '../datasets/large_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=27,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(75))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                   steps_per_epoch = 1,\n",
    "                   epochs = 1, validation_data = val_generator, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('../datasets/large_sample/almond_desert/6350.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "display(img)\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = model.predict(val_generator, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.argmax(y[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generator for Data augmentation. Edit possible\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_generator = val_datagen.flow_from_directory(\n",
    "        '../datasets/small_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_val_generator = val_datagen.flow_from_directory(\n",
    "        '../datasets/small_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:13]:\n",
    "    layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(small_train_generator,\n",
    "                   steps_per_epoch = 100,\n",
    "                   epochs = 1, validation_data = small_val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img('../datasets/small_sample/black_rice_cake/10585.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "display(img)\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsSmall = (small_train_generator.class_indices)\n",
    "labelsSmall = dict((v,k) for k,v in labelsSmall.items())\n",
    "print(labelsSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.argmax(y)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
