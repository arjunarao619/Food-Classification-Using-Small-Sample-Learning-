{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is an alternate way of making the input data pipeline using Keras Image Data Generator- flow_from_directory. Eases dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implementing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Implementation of custom metrics: Precision, Recall, F-Measure and Confusion Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        \n",
    "        for i in range(len(self.validation_data)):\n",
    "            x_val, y_val = self.validation_data.__getitem__(i)\n",
    "\n",
    "        print(y_val.shape)\n",
    "        print(y_val[1])\n",
    "        print(y_val[2])\n",
    "\n",
    "        y_predict = np.asarray(model.predict(self.validation_data, steps = 1))\n",
    "        print(y_predict.shape)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            lab = [i for i in range(0,75)]\n",
    "            print(y_val.shape)\n",
    "\n",
    "            y_val = np.argmax(y_val, axis=1)\n",
    "            y_predict = np.argmax(y_predict, axis=1)\n",
    "            print(\"y_val: \", y_val)\n",
    "            print(\"y_predict:\", y_predict)\n",
    "            print(\"\\nMetrics for Epoch\")\n",
    "            print(\"Confusion Matrix:\\n\",confusion_matrix(y_val,y_predict, labels=lab))\n",
    "            print(\"Recall: \", recall_score(y_val,y_predict, average=None, labels=lab))\n",
    "            print(\"Precision: \", precision_score(y_val,y_predict, average = None, labels=lab))\n",
    "            print(\"F1_score: \", f1_score(y_val,y_predict, average =None, labels=lab))\n",
    "            print(\"\\n\") \n",
    "            self._data.append({\n",
    "                'val_recall': recall_score(y_val, y_predict, average = None, labels=lab),\n",
    "                'val_precision': precision_score(y_val, y_predict, average = None, labels=lab),\n",
    "                'val_f1_score': f1_score(y_val,y_predict, average = None, labels=lab),\n",
    "            })\n",
    "            return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generator for Data augmentation. Edit possible\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.0017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "#Sanity check for augmentation\n",
    "\n",
    "img = load_img('../datasets/large_sample/almond_desert/6350.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "print(x.shape)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in train_datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='almond_desert', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45815 images belonging to 75 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../datasets/large_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 images belonging to 75 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = train_datagen.flow_from_directory(\n",
    "        '../datasets/large_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=27,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'almond_desert', 1: 'apple_pine', 2: 'baby_cabbage', 3: 'breakfast_burritos', 4: 'cake', 5: 'candied_haw', 6: 'chinese_mitten_crab', 7: 'clam_juice_chicken_feet', 8: 'coconut_cake', 9: 'coffee', 10: 'cool_noodles', 11: 'corn_cake', 12: 'donuts', 13: 'double_cooked_pork_slices', 14: 'dry_fried_noodle', 15: 'duck_neck', 16: 'dumpling_ham', 17: 'dumplings', 18: 'egg_pudding', 19: 'egg_tart', 20: 'egg_waffle', 21: 'egg_yolk_puff', 22: 'ferment_cake', 23: 'french_toast', 24: 'fried_dough_twists', 25: 'fried_rice', 26: 'fruit_salad', 27: 'grilled_cold_noodles', 28: 'grilled_fish', 29: 'gui_ling_jelly', 30: 'hong_kong_style_french_toast', 31: 'hot_and_sour_soup', 32: 'hot_pot', 33: 'huntun', 34: 'ice_cream', 35: 'mango_crepe', 36: 'mashed_potato', 37: 'milk_pudding', 38: 'millet_porridge', 39: 'mixed_porridge', 40: 'moon_cake', 41: 'pancake', 42: 'pickle', 43: 'pineapple_buns', 44: 'pizza', 45: 'pork_knuckle', 46: 'potato_shred', 47: 'puffs', 48: 'pumpkin_porridge', 49: 'red_bean_pudding', 50: 'red_swamp_crayfish', 51: 'rice_dumpling', 52: 'rice_noodle', 53: 'rice_porridge', 54: 'roast_duck', 55: 'salted_duck_egg', 56: 'sandwich', 57: 'sausage', 58: 'shark_fine_soup', 59: 'shrimp_dumplings', 60: 'sliced_cucumber', 61: 'sponge_cake', 62: 'squirrel_fish', 63: 'stall_noodle', 64: 'steamed_bread', 65: 'steamed_rice_roll', 66: 'sumai', 67: 'sushi', 68: 'takoyaki', 69: 'tang_yuan', 70: 'tofu', 71: 'turnip_cake', 72: 'walnut_cake', 73: 'wonton_noodles', 74: 'xiao_long_bao'}\n"
     ]
    }
   ],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(75))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                4875      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 75)                0         \n",
      "=================================================================\n",
      "Total params: 37,675\n",
      "Trainable params: 37,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "(27, 75)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "(27, 75)\n",
      "(27, 75)\n",
      "y_val:  [66 43  5 51 40 29 63 24 69  6 39 25 37  2 71 14 16 58 59 53 67 48  1 50\n",
      " 26 65 44]\n",
      "y_predict: [60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 60 66 60 60 60\n",
      " 60 60 60]\n",
      "\n",
      "Metrics for Epoch\n",
      "Confusion Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Recall:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Precision:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "F1_score:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "\n",
      "\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.3277 - acc: 0.0000e+00 - val_loss: 4.3146 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mudit/anaconda3/envs/TensorflowGPU/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/mudit/anaconda3/envs/TensorflowGPU/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mudit/anaconda3/envs/TensorflowGPU/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/mudit/anaconda3/envs/TensorflowGPU/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a076abb38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                   steps_per_epoch = 1,\n",
    "                   epochs = 1, validation_data = val_generator, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F3A161D2F98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "img = load_img('../datasets/large_sample/almond_desert/6350.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "display(img)\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = model.predict(val_generator, steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sliced_cucumber'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(y[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d\n",
      "1 activation\n",
      "2 max_pooling2d\n",
      "3 conv2d_1\n",
      "4 activation_1\n",
      "5 max_pooling2d_1\n",
      "6 conv2d_2\n",
      "7 activation_2\n",
      "8 max_pooling2d_2\n",
      "9 flatten\n",
      "10 dense\n",
      "11 activation_3\n",
      "12 dropout\n",
      "13 dense_1\n",
      "14 activation_4\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generator for Data augmentation. Edit possible\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "small_train_generator = val_datagen.flow_from_directory(\n",
    "        '../datasets/small_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "small_val_generator = val_datagen.flow_from_directory(\n",
    "        '../datasets/small_sample',  # this is the target directory\n",
    "        target_size=(28, 28),  # all images will be resized to 150x150\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset = 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:13]:\n",
    "    layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n",
      "5 False\n",
      "6 False\n",
      "7 False\n",
      "8 False\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 True\n",
      "14 True\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                4875      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1900      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 25)                0         \n",
      "=================================================================\n",
      "Total params: 39,575\n",
      "Trainable params: 6,775\n",
      "Non-trainable params: 32,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 3.2225 - acc: 0.0347 - val_loss: 3.2213 - val_acc: 0.0400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a0755ecc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(small_train_generator,\n",
    "                   steps_per_epoch = 100,\n",
    "                   epochs = 1, validation_data = small_val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F3A161D23C8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "img = load_img('../datasets/small_sample/black_rice_cake/10585.jpg')  # this is a PIL image\n",
    "img=img.resize((28,28))\n",
    "display(img)\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Omelette', 1: 'Snowflake_Flaky_Pastry', 2: 'black_rice_cake', 3: 'black_sesame_paste', 4: 'boiled_corn', 5: 'braised_pork', 6: 'cheese', 7: 'claypot', 8: 'french_fries', 9: 'fried_chestnuts', 10: 'frozen_lemon_tea', 11: 'glutinous_rice_ball_with_peanut_and_sesame_sweets', 12: 'glutinous_rice_dumpling', 13: 'green_bean_cake', 14: 'hot_dog', 15: 'layer_cake', 16: 'mango_pomelo_sago', 17: 'onion_rings', 18: 'persimmon_pancake', 19: 'poon_choi', 20: 'pop_corn', 21: 'sampan_porridge', 22: 'scrambled_eggs_with_tomatoes', 23: 'steak', 24: 'stir_fried_lettuce'}\n"
     ]
    }
   ],
   "source": [
    "labelsSmall = (small_train_generator.class_indices)\n",
    "labelsSmall = dict((v,k) for k,v in labelsSmall.items())\n",
    "print(labelsSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baby_cabbage'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[np.argmax(y)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
